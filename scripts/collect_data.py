#!/usr/bin/env python3
"""
GitHubãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
PRã€code frequencyã€contributionsãªã©ã‚’å–å¾—ã—ã€äººã”ã¨ãƒ»æœˆã”ã¨ã«é›†è¨ˆ
"""

import json
import os
import sys
import time
from datetime import datetime, timedelta
from dateutil import parser
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from github import Github
from github import Auth
from github.GithubException import GithubException, RateLimitExceededException
import pytz

# æŒ‡å®šæ—¥æ•°å‰ã®æ—¥ä»˜ã‚’å–å¾—
def get_start_date(days=365):
    """æŒ‡å®šæ—¥æ•°å‰ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 365æ—¥ = 1å¹´ï¼‰"""
    return datetime.now(pytz.UTC) - timedelta(days=days)

# æœˆã®ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆYYYY-MMå½¢å¼ï¼‰
def get_month_key(date):
    if isinstance(date, str):
        date = parser.parse(date)
    return date.strftime('%Y-%m')

# ç¾åœ¨ã®æœˆã®é–‹å§‹æ—¥ã‚’å–å¾—
def get_current_month_start():
    """ç¾åœ¨ã®æœˆã®é–‹å§‹æ—¥ã‚’å–å¾—"""
    now = datetime.now(pytz.UTC)
    return datetime(now.year, now.month, 1, tzinfo=pytz.UTC)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
def get_cache_path(owner, repo_name):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'cache')
    os.makedirs(cache_dir, exist_ok=True)
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_name = f"{owner}_{repo_name}".replace('/', '_').replace('\\', '_')
    return os.path.join(cache_dir, f"{safe_name}.json")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
def load_cache(cache_path):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âš ï¸  Failed to load cache: {e}")
    return None

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
def save_cache(cache_path, data):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"  âš ï¸  Failed to save cache: {e}")

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ
def check_rate_limit(github, resource_type='core'):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
    rate_limit = github.get_rate_limit()

    # PyGithubã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦æ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ã€ä¸¡æ–¹ã«å¯¾å¿œ
    if hasattr(rate_limit, 'resources'):
        # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        if resource_type == 'core':
            core_limit = rate_limit.resources.core
            remaining = core_limit.remaining
            reset_time = core_limit.reset
        else:
            search_limit = rate_limit.resources.search
            remaining = search_limit.remaining
            reset_time = search_limit.reset
    else:
        # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        if resource_type == 'core':
            remaining = rate_limit.core.remaining
            reset_time = rate_limit.core.reset
        else:
            remaining = rate_limit.search.remaining
            reset_time = rate_limit.search.reset

    if remaining < 10:  # æ®‹ã‚ŠãŒ10æœªæº€ã®å ´åˆ
        wait_time = (reset_time - datetime.now(pytz.UTC)).total_seconds() + 10
        if wait_time > 0:
            print(f"  âš ï¸  Rate limit low ({remaining} remaining). Waiting {int(wait_time)} seconds...")
            time.sleep(wait_time)

    return remaining

# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
def collect_repo_data(github, owner, repo_name, start_date, collect_reviews=False, collect_commit_stats=True, use_cache=True):
    """ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    print(f"\n{'='*60}")
    print(f"Collecting data for {owner}/{repo_name}...")
    start_time = time.time()

    cache_path = get_cache_path(owner, repo_name)
    current_month_start = get_current_month_start()
    cached_data = None

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºå®šåˆ†ã‚’èª­ã¿è¾¼ã¿
    if use_cache:
        cached_data = load_cache(cache_path)
        if cached_data:
            print(f"  ğŸ“¦ Loaded cache (last updated: {cached_data.get('cached_at', 'unknown')})")

    try:
        repo = github.get_repo(f"{owner}/{repo_name}")
    except GithubException as e:
        if e.status == 401:
            print(f"Error accessing {owner}/{repo_name}: Authentication failed (401)")
            print("  The token may not have access to this repository, or the token is invalid.")
        elif e.status == 403:
            print(f"Error accessing {owner}/{repo_name}: Access forbidden (403)")
            print("  The token may not have sufficient permissions, or the repository is private and the token lacks 'repo' scope.")
        elif e.status == 404:
            print(f"Error accessing {owner}/{repo_name}: Repository not found (404)")
            print("  Please check if the repository name and owner are correct.")
        else:
            print(f"Error accessing {owner}/{repo_name}: {e}")
        return None

    data = {
        'repository': f"{owner}/{repo_name}",
        'prs': [],
        'code_frequency': defaultdict(lambda: {'additions': 0, 'deletions': 0}),
        'contributions': defaultdict(lambda: {
            'commits': 0,
            'additions': 0,
            'deletions': 0,
            'prs_created': 0,
            'prs_merged': 0,
            'prs_reviewed': 0
        }),
        'monthly_stats': defaultdict(lambda: {
            'prs_created': 0,
            'prs_merged': 0,
            'additions': 0,
            'deletions': 0,
            'contributors': set()
        })
    }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºå®šåˆ†ã®PRã‚’èª­ã¿è¾¼ã¿
    cached_prs = []
    cached_contributions = defaultdict(lambda: {
        'commits': 0,
        'additions': 0,
        'deletions': 0,
        'prs_created': 0,
        'prs_merged': 0,
        'prs_reviewed': 0
    })
    cached_monthly_stats = defaultdict(lambda: {
        'prs_created': 0,
        'prs_merged': 0,
        'additions': 0,
        'deletions': 0,
        'contributors': set()
    })
    cached_code_frequency = defaultdict(lambda: {'additions': 0, 'deletions': 0})
    cached_devin_breakdown = defaultdict(lambda: {
        'prs_merged': 0,
        'additions': 0,
        'deletions': 0
    })

    if cached_data and use_cache:
        # ç¢ºå®šåˆ†ï¼ˆå½“æœˆã‚ˆã‚Šå‰ï¼‰ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        # start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        cache_has_old_data = False
        for cached_pr in cached_data.get('prs', []):
            pr_created = parser.parse(cached_pr['created_at'])
            if pr_created < current_month_start:
                cached_prs.append(cached_pr)
                # start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
                if pr_created >= start_date:
                    cache_has_old_data = True

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€é€šå¸¸é€šã‚Šå–å¾—ã™ã‚‹
        if not cache_has_old_data and len(cached_prs) > 0:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€ã‚‚å¤ã„PRã®æ—¥ä»˜ã‚’ç¢ºèª
            oldest_cached_pr_date = min(parser.parse(pr['created_at']) for pr in cached_prs)
            if oldest_cached_pr_date > start_date:
                print(f"  âš ï¸  Cache doesn't contain data before {start_date.strftime('%Y-%m-%d')}, will fetch from API")
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦é€šå¸¸é€šã‚Šå–å¾—
                cached_prs = []
                cached_contributions = defaultdict(lambda: {
                    'commits': 0,
                    'additions': 0,
                    'deletions': 0,
                    'prs_created': 0,
                    'prs_merged': 0,
                    'prs_reviewed': 0
                })
                cached_monthly_stats = defaultdict(lambda: {
                    'prs_created': 0,
                    'prs_merged': 0,
                    'additions': 0,
                    'deletions': 0,
                    'contributors': set()
                })
                cached_code_frequency = defaultdict(lambda: {'additions': 0, 'deletions': 0})
                cached_devin_breakdown = defaultdict(lambda: {
                    'prs_merged': 0,
                    'additions': 0,
                    'deletions': 0
                })

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿çµ±è¨ˆã‚’èª­ã¿è¾¼ã¿
        if len(cached_prs) > 0:
            # ç¢ºå®šåˆ†ã®çµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            for month, stats in cached_data.get('monthly_stats', {}).items():
                month_date = parser.parse(f"{month}-01")
                if month_date < current_month_start:
                    cached_monthly_stats[month] = stats.copy()
                    if isinstance(stats.get('contributors'), int):
                        cached_monthly_stats[month]['contributors'] = set()

            for month, freq in cached_data.get('code_frequency', {}).items():
                month_date = parser.parse(f"{month}-01")
                if month_date < current_month_start:
                    cached_code_frequency[month] = freq.copy()

            # ç¢ºå®šåˆ†ã®ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼çµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            for contributor, stats in cached_data.get('contributions', {}).items():
                cached_contributions[contributor] = stats.copy()

            # devin-botã®å†…è¨³ã‚‚èª­ã¿è¾¼ã¿
            for contributor, breakdown in cached_data.get('devin_breakdown', {}).items():
                cached_devin_breakdown[contributor] = breakdown.copy()

            print(f"  ğŸ“¦ Using {len(cached_prs)} cached PRs (before {current_month_start.strftime('%Y-%m')})")
        else:
            print(f"  ğŸ“¦ Cache cleared, will fetch all data from API")

    # PRãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆå½“æœˆåˆ†ã®ã¿ï¼‰
    try:
        check_rate_limit(github)
        prs = repo.get_pulls(state='all', sort='updated', direction='desc')

        pr_count = 0
        new_pr_count = 0
        last_progress_time = time.time()
        progress_interval = 60  # 60ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º
        total_checked = 0  # start_dateä»¥é™ã®PRã‚’ãƒã‚§ãƒƒã‚¯ã—ãŸæ•°

        for pr in prs:
            # ç›´è¿‘1å¹´é–“ã®PRã®ã¿å‡¦ç†
            if pr.updated_at < start_date:
                break

            total_checked += 1  # start_dateä»¥é™ã®PRã‚’ãƒã‚§ãƒƒã‚¯

            # ç¢ºå®šåˆ†ï¼ˆå½“æœˆã‚ˆã‚Šå‰ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
            # ãŸã ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã‚„ã€start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯å–å¾—ã™ã‚‹
            pr_created = pr.created_at
            if pr_created < current_month_start:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if len(cached_prs) > 0:
                    continue
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ã€start_dateä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                if pr_created < start_date:
                    continue

            # ãƒãƒ¼ã‚¸ã—ãŸäººã‚’å–å¾—ï¼ˆdevin-botã®PRã®å ´åˆã«ä½¿ç”¨ï¼‰
            merged_by = None
            if pr.merged_at and pr.merged_by:
                merged_by = pr.merged_by.login

            pr_data = {
                'number': pr.number,
                'title': pr.title,
                'author': pr.user.login if pr.user else 'unknown',
                'state': pr.state,
                'created_at': pr.created_at.isoformat(),
                'merged_at': pr.merged_at.isoformat() if pr.merged_at else None,
                'merged_by': merged_by,
                'additions': pr.additions,
                'deletions': pr.deletions,
                'reviewers': []
            }

            # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç„¡åŠ¹ï¼‰
            if collect_reviews:
                try:
                    check_rate_limit(github)
                    reviews = pr.get_reviews()
                    for review in reviews:
                        if review.user and review.user.login not in pr_data['reviewers']:
                            pr_data['reviewers'].append(review.user.login)
                except RateLimitExceededException:
                    print(f"  âš ï¸  Rate limit exceeded while fetching reviews for PR #{pr.number}, skipping...")
                    break
                except Exception:
                    pass  # ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

            data['prs'].append(pr_data)
            pr_count += 1
            new_pr_count += 1

            # é€²æ—è¡¨ç¤ºï¼ˆ1åˆ†ã”ã¨ï¼‰
            current_time = time.time()
            if current_time - last_progress_time >= progress_interval:
                elapsed = int(current_time - start_time)
                elapsed_min = elapsed // 60
                elapsed_sec = elapsed % 60

                # å‡¦ç†é€Ÿåº¦ã‚’è¨ˆç®—ï¼ˆPR/ç§’ï¼‰
                if elapsed > 0:
                    rate = new_pr_count / elapsed
                    # é€²æ—ç‡ã‚’è¨ˆç®—ï¼ˆtotal_checkedã‹ã‚‰æ¨å®šï¼‰
                    if total_checked > 0:
                        progress_pct = min(100, int((new_pr_count / total_checked) * 100))
                        # æ®‹ã‚Šæ™‚é–“ã‚’æ¨å®š
                        remaining = max(0, total_checked - new_pr_count)
                        if rate > 0 and remaining > 0:
                            eta_seconds = int(remaining / rate)
                            eta_min = eta_seconds // 60
                            eta_sec = eta_seconds % 60
                            if elapsed_min > 0:
                                print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s, ETA: ~{eta_min}m {eta_sec}s)")
                            else:
                                print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s, ETA: ~{eta_sec}s)")
                        else:
                            if elapsed_min > 0:
                                print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                            else:
                                print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                    else:
                        # total_checkedãŒ0ã®å ´åˆã¯å¾“æ¥ã®è¡¨ç¤º
                        if rate > 0:
                            if elapsed_min > 0:
                                print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                            else:
                                print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                        else:
                            if elapsed_min > 0:
                                print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_min}m {elapsed_sec}s)")
                            else:
                                print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_sec}s)")
                else:
                    print(f"  â³ Progress: {new_pr_count} PRs collected")
                last_progress_time = current_time

            # æœˆã”ã¨ã®çµ±è¨ˆ
            month_key = get_month_key(pr.created_at)
            data['monthly_stats'][month_key]['prs_created'] += 1
            if pr.merged_at:
                merge_month = get_month_key(pr.merged_at)
                data['monthly_stats'][merge_month]['prs_merged'] += 1

            # devin-ai-integration[bot]ã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã€å®Ÿç¸¾ã‚’ãƒãƒ¼ã‚¸ã—ãŸäººã«è¨ˆä¸Š
            author = pr.user.login if pr.user else 'unknown'
            is_devin_bot = author == 'devin-ai-integration[bot]'

            if is_devin_bot and pr.merged_at and merged_by:
                # devin-botã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã€ãƒãƒ¼ã‚¸ã—ãŸäººã«å®Ÿç¸¾ã‚’è¨ˆä¸Š
                merger = merged_by
                data['contributions'][merger]['prs_merged'] += 1
                data['contributions'][merger]['additions'] += pr.additions
                data['contributions'][merger]['deletions'] += pr.deletions
                data['monthly_stats'][merge_month]['contributors'].add(merger)

                # devin-botã®å†…è¨³ã‚‚è¨˜éŒ²ï¼ˆæ‹¬å¼§æ›¸ãè¡¨ç¤ºç”¨ï¼‰
                if 'devin_breakdown' not in data:
                    data['devin_breakdown'] = defaultdict(lambda: {
                        'prs_merged': 0,
                        'additions': 0,
                        'deletions': 0
                    })
                data['devin_breakdown'][merger]['prs_merged'] += 1
                data['devin_breakdown'][merger]['additions'] += pr.additions
                data['devin_breakdown'][merger]['deletions'] += pr.deletions
            else:
                # é€šå¸¸ã®PRã®çµ±è¨ˆ
                if pr.user:
                    data['contributions'][author]['prs_created'] += 1
                    if pr.merged_at:
                        data['contributions'][author]['prs_merged'] += 1
                    data['contributions'][author]['additions'] += pr.additions
                    data['contributions'][author]['deletions'] += pr.deletions
                    data['monthly_stats'][month_key]['contributors'].add(author)

            # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®çµ±è¨ˆ
            for reviewer in pr_data['reviewers']:
                data['contributions'][reviewer]['prs_reviewed'] += 1

        print(f"  âœ“ Collected {new_pr_count} new PRs (total: {pr_count + len(cached_prs)} with cache)")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã PRã‚’è¿½åŠ 
        data['prs'].extend(cached_prs)
    except RateLimitExceededException:
        print(f"  âš ï¸  Rate limit exceeded while fetching PRs")
    except Exception as e:
        print(f"  âœ— Error collecting PRs: {e}")

    # Code frequencyãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆã‚³ãƒŸãƒƒãƒˆçµ±è¨ˆï¼‰
    if collect_commit_stats:
        # ç¢ºå®šåˆ†ã®ã‚³ãƒŸãƒƒãƒˆçµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        if cached_data and use_cache:
            for month, freq in cached_code_frequency.items():
                data['code_frequency'][month] = freq.copy()

        try:
            check_rate_limit(github)
            commits = repo.get_commits(since=start_date)
            commit_count = 0
            new_commit_count = 0
            max_commits = 1000  # APIåˆ¶é™ã‚’è€ƒæ…®
            stats_errors = 0
            last_commit_progress_time = time.time()
            commit_progress_interval = 60  # 60ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º

            for commit in commits:
                commit_count += 1
                if commit_count > max_commits:
                    print(f"  âš ï¸  Reached commit limit ({max_commits}), stopping collection")
                    break

                try:
                    commit_date = commit.commit.author.date

                    # æ—¥ä»˜ãŒç¯„å›²å¤–ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if commit_date < start_date:
                        continue

                    # ç¢ºå®šåˆ†ï¼ˆå½“æœˆã‚ˆã‚Šå‰ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
                    # ãŸã ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã‚„ã€start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯å–å¾—ã™ã‚‹
                    if commit_date < current_month_start:
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if len(cached_code_frequency) > 0:
                            continue
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ã€start_dateä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        if commit_date < start_date:
                            continue

                    # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆé‡ã„APIå‘¼ã³å‡ºã—ï¼‰
                    # ã‚¨ãƒ©ãƒ¼ãŒå¤šã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if stats_errors < 10:
                        try:
                            check_rate_limit(github)
                            stats = commit.stats
                            additions = stats.additions
                            deletions = stats.deletions
                        except RateLimitExceededException:
                            print(f"  âš ï¸  Rate limit exceeded while fetching commit stats, stopping...")
                            break
                        except Exception:
                            stats_errors += 1
                            additions = 0
                            deletions = 0
                    else:
                        # çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼ãŒå¤šã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        additions = 0
                        deletions = 0

                    month_key = get_month_key(commit_date)
                    data['code_frequency'][month_key]['additions'] += additions
                    data['code_frequency'][month_key]['deletions'] += deletions

                    # ã‚³ãƒŸãƒƒãƒˆä½œæˆè€…ã®çµ±è¨ˆ
                    if commit.author:
                        author = commit.author.login
                        data['contributions'][author]['commits'] += 1
                        data['contributions'][author]['additions'] += additions
                        data['contributions'][author]['deletions'] += deletions
                        data['monthly_stats'][month_key]['contributors'].add(author)

                    # æœˆã”ã¨ã®çµ±è¨ˆ
                    data['monthly_stats'][month_key]['additions'] += additions
                    data['monthly_stats'][month_key]['deletions'] += deletions
                    new_commit_count += 1

                    # é€²æ—è¡¨ç¤ºï¼ˆ1åˆ†ã”ã¨ï¼‰
                    current_time = time.time()
                    if current_time - last_commit_progress_time >= commit_progress_interval:
                        elapsed = int(current_time - start_time)
                        elapsed_min = elapsed // 60
                        elapsed_sec = elapsed % 60

                        # å‡¦ç†é€Ÿåº¦ã‚’è¨ˆç®—ï¼ˆã‚³ãƒŸãƒƒãƒˆ/ç§’ï¼‰
                        if elapsed > 0:
                            rate = new_commit_count / elapsed
                            # æ®‹ã‚Šæ™‚é–“ã‚’æ¨å®š
                            if rate > 0:
                                # æ®‹ã‚Šã®ã‚³ãƒŸãƒƒãƒˆæ•°ã‚’æ¨å®šï¼ˆæœ€å¤§1000ä»¶ã¾ã§ï¼‰
                                remaining_estimate = max(0, max_commits - commit_count)
                                if remaining_estimate > 0:
                                    eta_seconds = int(remaining_estimate / rate) if rate > 0 else 0
                                    eta_min = eta_seconds // 60
                                    eta_sec = eta_seconds % 60
                                    progress_pct = min(100, int((commit_count / max_commits) * 100)) if max_commits > 0 else 0
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_commit_count} commits processed ({progress_pct}%, elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} commits/s, ETA: ~{eta_min}m {eta_sec}s)")
                                    else:
                                        print(f"  â³ Progress: {new_commit_count} commits processed ({progress_pct}%, elapsed: {elapsed_sec}s, rate: {rate:.2f} commits/s, ETA: ~{eta_sec}s)")
                                else:
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_commit_count} commits processed (elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} commits/s)")
                                    else:
                                        print(f"  â³ Progress: {new_commit_count} commits processed (elapsed: {elapsed_sec}s, rate: {rate:.2f} commits/s)")
                            else:
                                if elapsed_min > 0:
                                    print(f"  â³ Progress: {new_commit_count} commits processed (elapsed: {elapsed_min}m {elapsed_sec}s)")
                                else:
                                    print(f"  â³ Progress: {new_commit_count} commits processed (elapsed: {elapsed_sec}s)")
                        else:
                            print(f"  â³ Progress: {new_commit_count} commits processed")
                        last_commit_progress_time = current_time
                except Exception as e:
                    # å€‹åˆ¥ã®ã‚³ãƒŸãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                    continue

            print(f"  âœ“ Collected {new_commit_count} new commits (total: {commit_count} with cache)")
            if stats_errors > 0:
                print(f"  âš ï¸  Skipped stats for {stats_errors} commits due to errors")
        except RateLimitExceededException:
            print(f"  âš ï¸  Rate limit exceeded while fetching commits")
        except Exception as e:
            print(f"  âœ— Error collecting commits: {e}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã çµ±è¨ˆã‚’ãƒãƒ¼ã‚¸
    for contributor, stats in cached_contributions.items():
        for key, value in stats.items():
            data['contributions'][contributor][key] += value

    for month, stats in cached_monthly_stats.items():
        for key, value in stats.items():
            if key == 'contributors':
                if isinstance(value, set):
                    data['monthly_stats'][month]['contributors'].update(value)
                else:
                    # æ•°å€¤ã®å ´åˆã¯ç„¡è¦–ï¼ˆå¾Œã§è¨ˆç®—ã—ç›´ã™ï¼‰
                    pass
            else:
                data['monthly_stats'][month][key] += value

    for contributor, breakdown in cached_devin_breakdown.items():
        if 'devin_breakdown' not in data:
            data['devin_breakdown'] = defaultdict(lambda: {
                'prs_merged': 0,
                'additions': 0,
                'deletions': 0
            })
        for key, value in breakdown.items():
            data['devin_breakdown'][contributor][key] += value

    # ã‚»ãƒƒãƒˆã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
    for month_key in data['monthly_stats']:
        if isinstance(data['monthly_stats'][month_key]['contributors'], set):
            data['monthly_stats'][month_key]['contributors'] = len(data['monthly_stats'][month_key]['contributors'])

    # è¾æ›¸ã‚’é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›
    data['code_frequency'] = dict(data['code_frequency'])
    data['contributions'] = dict(data['contributions'])
    data['monthly_stats'] = dict(data['monthly_stats'])
    if 'devin_breakdown' in data:
        data['devin_breakdown'] = dict(data['devin_breakdown'])

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®ãŸã‚ã«ï¼‰
    if use_cache:
        cache_data = {
            'cached_at': datetime.now(pytz.UTC).isoformat(),
            'repository': data['repository'],
            'prs': data['prs'],
            'contributions': data['contributions'],
            'monthly_stats': data['monthly_stats'],
            'code_frequency': data['code_frequency'],
            'devin_breakdown': data.get('devin_breakdown', {})
        }
        save_cache(cache_path, cache_data)
        print(f"  ğŸ’¾ Cache saved for next run")

    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    if minutes > 0:
        print(f"  âœ“ Completed in {minutes}m {seconds}s")
    else:
        print(f"  âœ“ Completed in {elapsed_time:.1f}s")
    print(f"{'='*60}\n")

    return data

def main():
    # GitHub PATã‚’å–å¾—
    github_token = os.getenv('GITHUB_TOKEN')
    if not github_token:
        print("Error: GITHUB_TOKEN environment variable is not set")
        print("Please set GITHUB_TOKEN environment variable or use GitHub Actions secrets")
        print("You can create a token at: https://github.com/settings/tokens")
        sys.exit(1)

    # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦Githubã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆæ–°ã—ã„APIã‚’ä½¿ç”¨ï¼‰
    try:
        auth = Auth.Token(github_token)
        github = Github(auth=auth, per_page=100)

        # èªè¨¼ã‚’ãƒ†ã‚¹ãƒˆ
        user = github.get_user()
        print(f"Authenticated as: {user.login}")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤º
        rate_limit = github.get_rate_limit()
        if hasattr(rate_limit, 'resources'):
            core_limit = rate_limit.resources.core
            print(f"Rate limit: {core_limit.remaining}/{core_limit.limit} (resets at {core_limit.reset})")
        else:
            print(f"Rate limit: {rate_limit.core.remaining}/{rate_limit.core.limit} (resets at {rate_limit.core.reset})")
    except GithubException as e:
        if e.status == 401:
            print("Error: Invalid GitHub token (401 Unauthorized)")
            print("Please check your GITHUB_TOKEN:")
            print("1. Token is valid and not expired")
            print("2. Token has necessary permissions (repo scope for private repos)")
            print("3. Token is correctly set in environment variable")
            print("You can create a new token at: https://github.com/settings/tokens")
        else:
            print(f"Error authenticating with GitHub: {e}")
        sys.exit(1)

    # ãƒªãƒã‚¸ãƒˆãƒªè¨­å®šã‚’èª­ã¿è¾¼ã¿
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'repos.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿
    options = config.get('options', {})
    collect_reviews = options.get('collect_reviews', False)
    collect_commit_stats = options.get('collect_commit_stats', True)
    max_workers = options.get('max_workers', 3)
    use_cache = options.get('use_cache', True)

    # å¯¾è±¡æœŸé–“ã®è¨­å®š
    # days: ä½•æ—¥å‰ã‹ã‚‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 365æ—¥ = 1å¹´ï¼‰
    # start_date: é–‹å§‹æ—¥ã‚’ISOå½¢å¼ã§æŒ‡å®šï¼ˆä¾‹: "2024-01-01T00:00:00Z"ï¼‰
    # start_dateãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆ
    if 'start_date' in options:
        try:
            start_date = parser.parse(options['start_date'])
            if start_date.tzinfo is None:
                start_date = pytz.UTC.localize(start_date)
            print(f"Using custom start date: {start_date.isoformat()}")
        except Exception as e:
            print(f"Warning: Invalid start_date format, using days option instead: {e}")
            days = options.get('days', 365)
            start_date = get_start_date(days)
    else:
        days = options.get('days', 365)
        start_date = get_start_date(days)
        print(f"Using {days} days period (from {start_date.isoformat()})")

    all_data = []

    repos = config['repositories']
    total_repos = len(repos)

    print(f"\n{'='*60}")
    print(f"Processing {total_repos} repository/repositories...")
    print(f"Options: collect_reviews={collect_reviews}, collect_commit_stats={collect_commit_stats}, max_workers={max_workers}, use_cache={use_cache}")
    print(f"Period: {start_date.isoformat()} to {datetime.now(pytz.UTC).isoformat()}")
    print(f"{'='*60}\n")

    # ä¸¦åˆ—å‡¦ç†ã§å„ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    if total_repos > 1 and max_workers > 1:
        print(f"Using parallel processing (max {max_workers} workers)...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
            future_to_repo = {
                executor.submit(
                    collect_repo_data,
                    github,
                    repo_config['owner'],
                    repo_config['name'],
                    start_date,
                    collect_reviews,
                    collect_commit_stats,
                    use_cache
                ): repo_config
                for repo_config in repos
            }

            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
            for future in as_completed(future_to_repo):
                repo_config = future_to_repo[future]
                try:
                    repo_data = future.result()
                    if repo_data:
                        all_data.append(repo_data)
                except Exception as e:
                    print(f"Error processing {repo_config['owner']}/{repo_config['name']}: {e}")
    else:
        # é †æ¬¡å‡¦ç†ï¼ˆå˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªã¾ãŸã¯ä¸¦åˆ—å‡¦ç†ãŒç„¡åŠ¹ãªå ´åˆï¼‰
        for repo_config in repos:
            owner = repo_config['owner']
            name = repo_config['name']
            repo_data = collect_repo_data(github, owner, name, start_date, collect_reviews, collect_commit_stats, use_cache)
            if repo_data:
                all_data.append(repo_data)

    # ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'collected_data.json')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'collected_at': datetime.now(pytz.UTC).isoformat(),
            'start_date': start_date.isoformat(),
            'repositories': all_data
        }, f, indent=2, ensure_ascii=False)

    print(f"\n{'='*60}")
    print(f"Data collection completed. Saved to {output_path}")
    print(f"Total repositories processed: {len(all_data)}/{total_repos}")

    # æœ€çµ‚çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤º
    rate_limit = github.get_rate_limit()
    if hasattr(rate_limit, 'resources'):
        core_limit = rate_limit.resources.core
        print(f"Rate limit remaining: {core_limit.remaining}/{core_limit.limit}")
    else:
        print(f"Rate limit remaining: {rate_limit.core.remaining}/{rate_limit.core.limit}")

if __name__ == '__main__':
    main()
