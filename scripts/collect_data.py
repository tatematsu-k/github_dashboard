#!/usr/bin/env python3
"""
GitHubãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
PRã€code frequencyã€contributionsãªã©ã‚’å–å¾—ã—ã€äººã”ã¨ãƒ»æœˆã”ã¨ã«é›†è¨ˆ
"""

import json
import os
import sys
import time
import requests
from datetime import datetime, timedelta
from dateutil import parser
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from github import Github
from github import Auth
from github.GithubException import GithubException, RateLimitExceededException
import pytz

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®šï¼ˆJSTï¼‰
JST = pytz.timezone('Asia/Tokyo')

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ã‚­ãƒ¼ãƒã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¸Šã’ã‚‹
# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ç„¡è¦–ã•ã‚Œã€å…¨ã¦ä½œã‚Šç›´ã•ã‚Œã‚‹
# Version 2: æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯æ§‹é€ ã«å¤‰æ›´ï¼ˆstart_date/end_dateä»˜ãï¼‰
CACHE_SCHEMA_VERSION = 2

# æŒ‡å®šæ—¥æ•°å‰ã®æ—¥ä»˜ã‚’å–å¾—
def get_start_date(days=365):
    """æŒ‡å®šæ—¥æ•°å‰ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 365æ—¥ = 1å¹´ï¼‰"""
    return datetime.now(JST) - timedelta(days=days)

# æœˆã®ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆYYYY-MMå½¢å¼ï¼‰
def get_month_key(date):
    if isinstance(date, str):
        date = parser.parse(date)
    return date.strftime('%Y-%m')

# é€±ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆYYYY-WWå½¢å¼ã€ISOé€±ç•ªå·ï¼‰
def get_week_key(date):
    """é€±ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆYYYY-WWå½¢å¼ã€ISOé€±ç•ªå·ï¼‰"""
    if isinstance(date, str):
        date = parser.parse(date)
    # ISOé€±ç•ªå·ã‚’å–å¾—
    year, week, _ = date.isocalendar()
    return f"{year}-W{week:02d}"

# ç¾åœ¨ã®æœˆã®é–‹å§‹æ—¥ã‚’å–å¾—
def get_current_month_start():
    """ç¾åœ¨ã®æœˆã®é–‹å§‹æ—¥ã‚’å–å¾—"""
    now = datetime.now(JST)
    return datetime(now.year, now.month, 1, tzinfo=JST)

# ç¾åœ¨ã®é€±ã®é–‹å§‹æ—¥ã‚’å–å¾—ï¼ˆæœˆæ›œæ—¥ï¼‰
def get_current_week_start():
    """ç¾åœ¨ã®é€±ã®é–‹å§‹æ—¥ã‚’å–å¾—ï¼ˆæœˆæ›œæ—¥ï¼‰"""
    now = datetime.now(JST)
    # æœˆæ›œæ—¥ã‚’å–å¾—ï¼ˆ0=æœˆæ›œæ—¥ã€6=æ—¥æ›œæ—¥ï¼‰
    days_since_monday = now.weekday()
    return now - timedelta(days=days_since_monday)

# æœˆã®é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ã‚’å–å¾—
def get_month_range(year, month):
    """æŒ‡å®šã•ã‚ŒãŸå¹´æœˆã®é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ã‚’å–å¾—"""
    if month == 12:
        next_month = datetime(year + 1, 1, 1, tzinfo=JST)
    else:
        next_month = datetime(year, month + 1, 1, tzinfo=JST)
    month_start = datetime(year, month, 1, tzinfo=JST)
    month_end = next_month - timedelta(seconds=1)
    return month_start, month_end

# æœˆã‚­ãƒ¼ã‹ã‚‰å¹´æœˆã‚’å–å¾—
def parse_month_key(month_key):
    """æœˆã‚­ãƒ¼ï¼ˆYYYY-MMï¼‰ã‹ã‚‰å¹´ã¨æœˆã‚’å–å¾—"""
    year, month = map(int, month_key.split('-'))
    return year, month

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
def get_cache_path(owner, repo_name):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'cache')
    os.makedirs(cache_dir, exist_ok=True)
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_name = f"{owner}_{repo_name}".replace('/', '_').replace('\\', '_')
    return os.path.join(cache_dir, f"{safe_name}.json")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
def load_cache(cache_path):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
            cached_version = cached_data.get('schema_version', 0)
            if cached_version != CACHE_SCHEMA_VERSION:
                print(f"  âš ï¸  Cache schema version mismatch (cached: {cached_version}, current: {CACHE_SCHEMA_VERSION})")
                print(f"  ğŸ”„ Cache will be ignored and rebuilt")
                return None

            return cached_data
        except Exception as e:
            print(f"  âš ï¸  Failed to load cache: {e}")
    return None

# æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
def save_monthly_chunk(cache_path, month_key, chunk_data):
    """æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"""
    try:
        cache_dir = os.path.dirname(cache_path)
        os.makedirs(cache_dir, exist_ok=True)
        base_name = os.path.basename(cache_path).replace('.json', '')
        chunk_file = os.path.join(cache_dir, f"{base_name}_chunk_{month_key}.json")
        chunk_data['schema_version'] = CACHE_SCHEMA_VERSION
        with open(chunk_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ’¾ Saved chunk for {month_key} to {chunk_file}")
    except Exception as e:
        print(f"  âš ï¸  Failed to save monthly chunk for {month_key}: {e}")
        import traceback
        traceback.print_exc()

# æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿
def load_monthly_chunk(cache_path, month_key):
    """æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿"""
    try:
        cache_dir = os.path.dirname(cache_path)
        chunk_file = os.path.join(cache_dir, f"{os.path.basename(cache_path).replace('.json', '')}_chunk_{month_key}.json")
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
            cached_version = chunk_data.get('schema_version', 0)
            if cached_version != CACHE_SCHEMA_VERSION:
                return None
            return chunk_data
    except Exception as e:
        pass
    return None

# PRã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ï¼ˆç¢ºå®šåˆ†ã®PRã‚’è¿½åŠ ä¿å­˜ï¼‰
def update_pr_cache(cache_path, new_prs, start_date):
    """ç¢ºå®šåˆ†ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ ä¿å­˜ï¼ˆå‡¦ç†ä¸­æ–­ã«å‚™ãˆã‚‹ï¼‰"""
    try:
        # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
        cached_data = load_cache(cache_path)
        if not cached_data:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
            cached_data = {
                'schema_version': CACHE_SCHEMA_VERSION,
                'cached_at': datetime.now(JST).isoformat(),
                'start_date': start_date.isoformat(),
                'prs': [],
                'contributions': {},
                'monthly_stats': {},
                'monthly_contributions': {},
                'code_frequency': {},
                'devin_breakdown': {}
            }

        # æ–°ã—ã„PRã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        existing_pr_numbers = {pr['number'] for pr in cached_data.get('prs', [])}
        for pr in new_prs:
            if pr['number'] not in existing_pr_numbers:
                cached_data['prs'].append(pr)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        cached_data['cached_at'] = datetime.now(JST).isoformat()
        cached_data['schema_version'] = CACHE_SCHEMA_VERSION
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cached_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"  âš ï¸  Failed to update PR cache: {e}")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
def save_cache(cache_path, data):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ä»˜ãï¼‰"""
    try:
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
        data['schema_version'] = CACHE_SCHEMA_VERSION
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"  âš ï¸  Failed to save cache: {e}")

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ
def check_rate_limit(github, resource_type='core'):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
    rate_limit = github.get_rate_limit()

    # PyGithubã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦æ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ã€ä¸¡æ–¹ã«å¯¾å¿œ
    if hasattr(rate_limit, 'resources'):
        # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        if resource_type == 'core':
            core_limit = rate_limit.resources.core
            remaining = core_limit.remaining
            reset_time = core_limit.reset
        else:
            search_limit = rate_limit.resources.search
            remaining = search_limit.remaining
            reset_time = search_limit.reset
    else:
        # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        if resource_type == 'core':
            remaining = rate_limit.core.remaining
            reset_time = rate_limit.core.reset
        else:
            remaining = rate_limit.search.remaining
            reset_time = rate_limit.search.reset

    if remaining < 10:  # æ®‹ã‚ŠãŒ10æœªæº€ã®å ´åˆ
        wait_time = (reset_time - datetime.now(JST)).total_seconds() + 10
        if wait_time > 0:
            print(f"  âš ï¸  Rate limit low ({remaining} remaining). Waiting {int(wait_time)} seconds...")
            time.sleep(wait_time)

    return remaining

# GraphQLã§PRã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¸€æ‹¬å–å¾—
def fetch_prs_with_graphql(github_token, owner, repo_name, start_date, collect_reviews=True):
    """GraphQL APIã‚’ä½¿ç”¨ã—ã¦PRã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã‚’ä¸€æ‹¬å–å¾—"""
    graphql_url = "https://api.github.com/graphql"
    headers = {
        "Authorization": f"Bearer {github_token}",
        "Content-Type": "application/json"
    }

    all_prs = []
    cursor = None
    has_next_page = True

    # start_dateã‚’ISOå½¢å¼ã«å¤‰æ›
    start_date_str = start_date.isoformat()

    while has_next_page:
        # GraphQLã‚¯ã‚¨ãƒª
        query = """
        query($owner: String!, $repo: String!, $cursor: String) {
          repository(owner: $owner, name: $repo) {
            pullRequests(
              first: 100
              states: [OPEN, CLOSED, MERGED]
              orderBy: {field: CREATED_AT, direction: DESC}
              after: $cursor
            ) {
              nodes {
                number
                title
                author {
                  login
                }
                state
                createdAt
                mergedAt
                mergedBy {
                  login
                }
                additions
                deletions
                updatedAt
                reviews(first: 100) {
                  nodes {
                    author {
                      login
                    }
                  }
                }
              }
              pageInfo {
                hasNextPage
                endCursor
              }
            }
          }
          rateLimit {
            remaining
            resetAt
          }
        }
        """

        variables = {
            "owner": owner,
            "repo": repo_name,
            "cursor": cursor
        }

        payload = {
            "query": query,
            "variables": variables
        }

        try:
            response = requests.post(graphql_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()

            if "errors" in data:
                print(f"  âš ï¸  GraphQL errors: {data['errors']}")
                break

            repository = data.get("data", {}).get("repository")
            if not repository:
                print(f"  âš ï¸  Repository not found in GraphQL response")
                break

            pull_requests = repository.get("pullRequests", {})
            nodes = pull_requests.get("nodes", [])
            page_info = pull_requests.get("pageInfo", {})

            # ãƒ‡ãƒãƒƒã‚°: å–å¾—ã—ãŸãƒãƒ¼ãƒ‰æ•°ã‚’å‡ºåŠ›
            if cursor is None:
                print(f"  ğŸ” GraphQL: Received {len(nodes)} PR nodes from API")

            # start_dateã‚’UTCã«å¤‰æ›ã—ã¦æ¯”è¼ƒï¼ˆstart_dateãŒJSTã®å ´åˆã¯UTCã«å¤‰æ›ï¼‰
            start_date_utc = start_date
            if start_date.tzinfo == JST:
                start_date_utc = start_date.astimezone(pytz.UTC)
            elif start_date.tzinfo is None:
                start_date_utc = pytz.UTC.localize(start_date)

            if cursor is None:
                print(f"  ğŸ” Start date (UTC): {start_date_utc}")

            # PRã‚’å‡¦ç†
            nodes_processed = 0
            nodes_skipped_before_start = 0
            nodes_added = 0

            for pr_node in nodes:
                nodes_processed += 1
                created_at_str = pr_node.get("createdAt", "")
                if not created_at_str:
                    continue

                created_at = parser.parse(created_at_str)
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãŒNoneã®å ´åˆã¯UTCã¨ã—ã¦æ‰±ã†
                if created_at.tzinfo is None:
                    created_at = pytz.UTC.localize(created_at)

                # created_atãŒstart_dateã‚ˆã‚Šå‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if created_at < start_date_utc:
                    nodes_skipped_before_start += 1
                    # æœ€åˆã®PRãŒstart_dateã‚ˆã‚Šå‰ã®å ´åˆã¯ã€ä»¥é™ã‚‚å…¨ã¦å¤ã„PRãªã®ã§fetchã‚’åœæ­¢
                    if nodes_processed == 1:
                        print(f"  âš ï¸  First PR createdAt ({created_at}) < start_date_utc ({start_date_utc}), stopping pagination")
                        has_next_page = False
                        break
                    continue

                nodes_added += 1

                # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ãƒªã‚¹ãƒˆã‚’å–å¾—
                reviewers = []
                if collect_reviews:
                    reviews = pr_node.get("reviews", {}).get("nodes", [])
                    reviewer_set = set()
                    for review in reviews:
                        author = review.get("author", {})
                        if author and author.get("login"):
                            reviewer_set.add(author["login"])
                    reviewers = list(reviewer_set)

                merged_at = pr_node.get("mergedAt")
                merged_by_node = pr_node.get("mergedBy")
                merged_by = merged_by_node.get("login") if merged_by_node and merged_by_node.get("login") else None

                # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®æ•°ä»¶ã®PRã®mergedAtæƒ…å ±ã‚’å‡ºåŠ›
                if nodes_added <= 3:
                    print(f"  ğŸ” PR #{pr_node.get('number')}: state={pr_node.get('state')}, mergedAt={merged_at}, mergedBy={merged_by}")

                # stateã‚’å°æ–‡å­—ã«å¤‰æ›ï¼ˆMERGEDã‚‚å«ã‚€ï¼‰
                state = pr_node.get("state", "").lower()

                pr_data = {
                    "number": pr_node.get("number"),
                    "title": pr_node.get("title", ""),
                    "author": pr_node.get("author", {}).get("login", "unknown") if pr_node.get("author") else "unknown",
                    "state": state,
                    "created_at": created_at_str,
                    "merged_at": merged_at,
                    "merged_by": merged_by,
                    "additions": pr_node.get("additions", 0),
                    "deletions": pr_node.get("deletions", 0),
                    "reviewers": reviewers
                }

                all_prs.append(pr_data)

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
            if cursor is None:
                print(f"  ğŸ” Debug: Processed {nodes_processed} nodes, added {nodes_added}, skipped {nodes_skipped_before_start} (before start_date)")
                if nodes_processed > 0 and nodes_added == 0:
                    # æœ€åˆã®PRã®æƒ…å ±ã‚’å‡ºåŠ›ã—ã¦ãƒ‡ãƒãƒƒã‚°
                    first_pr = nodes[0] if nodes else None
                    if first_pr:
                        first_created = parser.parse(first_pr.get("createdAt", ""))
                        if first_created.tzinfo is None:
                            first_created = pytz.UTC.localize(first_created)
                        print(f"  ğŸ” First PR: created_at={first_created}, start_date_utc={start_date_utc}, diff={(first_created - start_date_utc).total_seconds() / 86400:.1f} days")

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ—¢ã«has_next_pageãŒFalseã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„ï¼‰
            if has_next_page:
                has_next_page = page_info.get("hasNextPage", False)
            cursor = page_info.get("endCursor")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            rate_limit = data.get("data", {}).get("rateLimit", {})
            remaining = rate_limit.get("remaining", 0)
            if remaining < 10:
                reset_at = rate_limit.get("resetAt")
                if reset_at:
                    reset_time = parser.parse(reset_at)
                    wait_time = (reset_time - datetime.now(JST)).total_seconds() + 10
                    if wait_time > 0:
                        print(f"  âš ï¸  GraphQL rate limit low ({remaining} remaining). Waiting {int(wait_time)} seconds...")
                        time.sleep(wait_time)

        except requests.exceptions.RequestException as e:
            print(f"  âš ï¸  GraphQL request error: {e}")
            import traceback
            print(f"  âš ï¸  Traceback: {traceback.format_exc()}")
            break
        except Exception as e:
            print(f"  âš ï¸  GraphQL error: {e}")
            import traceback
            print(f"  âš ï¸  Traceback: {traceback.format_exc()}")
            break

    print(f"  ğŸ” GraphQL: Total PRs collected: {len(all_prs)}")
    if len(all_prs) == 0 and cursor is None:
        print(f"  âš ï¸  WARNING: No PRs collected from GraphQL API. Check if repository has PRs or if filtering is too strict.")
    return all_prs

# PRã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰- å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™
def fetch_pr_reviews(github, pr_number, pr):
    """PRã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ã—ã¦ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        check_rate_limit(github)
        reviews = pr.get_reviews()
        reviewers = []
        for review in reviews:
            if review.user and review.user.login not in reviewers:
                reviewers.append(review.user.login)
        return pr_number, reviewers
    except RateLimitExceededException:
        return pr_number, []
    except Exception:
        return pr_number, []

# æœˆã”ã¨ã®ã‚³ãƒŸãƒƒãƒˆã‚’ãƒ•ã‚§ãƒƒãƒï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
def fetch_month_commits(github, owner, repo_name, month_key, month_start, month_end, cache_path, use_cache=True):
    """æœˆã”ã¨ã®ã‚³ãƒŸãƒƒãƒˆã‚’ãƒ•ã‚§ãƒƒãƒã—ã¦çµæœã‚’è¿”ã™"""
    print(f"  ğŸ”„ [{owner}/{repo_name} {month_key}] Starting commit fetch...")
    try:
        check_rate_limit(github)
        repo = github.get_repo(f"{owner}/{repo_name}")
        commits = repo.get_commits(since=month_start, until=month_end)

        month_code_frequency = defaultdict(lambda: {'additions': 0, 'deletions': 0})
        month_contributions = defaultdict(lambda: {
            'commits': 0,
            'additions': 0,
            'deletions': 0
        })
        month_monthly_contributions = defaultdict(lambda: {
            'commits': 0,
            'additions': 0,
            'deletions': 0
        })
        month_contributors = set()
        month_stats_errors = 0
        month_commit_count = 0

        for commit in commits:
            month_commit_count += 1
            try:
                commit_date = commit.commit.author.date

                # æœˆã®ç¯„å›²å¤–ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if commit_date < month_start or commit_date > month_end:
                    continue

                # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                if month_stats_errors < 10:
                    try:
                        check_rate_limit(github)
                        stats = commit.stats
                        additions = stats.additions
                        deletions = stats.deletions
                    except RateLimitExceededException:
                        print(f"  âš ï¸  [{owner}/{repo_name} {month_key}] Rate limit exceeded, stopping...")
                        break
                    except Exception:
                        month_stats_errors += 1
                        additions = 0
                        deletions = 0
                else:
                    additions = 0
                    deletions = 0

                month_code_frequency[month_key]['additions'] += additions
                month_code_frequency[month_key]['deletions'] += deletions

                # ã‚³ãƒŸãƒƒãƒˆä½œæˆè€…ã®çµ±è¨ˆ
                if commit.author:
                    author = commit.author.login
                    month_contributions[author]['commits'] += 1
                    month_contributions[author]['additions'] += additions
                    month_contributions[author]['deletions'] += deletions
                    month_monthly_contributions[author]['commits'] += 1
                    month_monthly_contributions[author]['additions'] += additions
                    month_monthly_contributions[author]['deletions'] += deletions
                    month_contributors.add(author)

            except Exception as e:
                continue

        # æœˆã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ï¼ˆã‚³ãƒŸãƒƒãƒˆãŒ1ä»¶ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if use_cache and month_commit_count > 0:
            chunk_data = {
                'start_date': month_start.isoformat(),
                'end_date': month_end.isoformat(),
                'code_frequency': {month_key: dict(month_code_frequency[month_key])},
                'monthly_stats': {month_key: {
                    'prs_created': 0,
                    'prs_merged': 0,
                    'additions': month_code_frequency[month_key]['additions'],
                    'deletions': month_code_frequency[month_key]['deletions'],
                    'contributors': list(month_contributors)
                }},
                'monthly_contributions': {month_key: {k: dict(v) for k, v in month_monthly_contributions.items()}},
                'contributions': {k: dict(v) for k, v in month_contributions.items()}
            }
            save_monthly_chunk(cache_path, month_key, chunk_data)
        elif month_commit_count == 0:
            print(f"  â„¹ï¸  [{owner}/{repo_name} {month_key}] No commits found, skipping chunk save")

        return {
            'month_key': month_key,
            'commit_count': month_commit_count,
            'code_frequency': dict(month_code_frequency),
            'contributions': {k: dict(v) for k, v in month_contributions.items()},
            'monthly_contributions': {month_key: {k: dict(v) for k, v in month_monthly_contributions.items()}},
            'contributors': list(month_contributors)
        }
    except RateLimitExceededException:
        print(f"  âš ï¸  [{owner}/{repo_name} {month_key}] Rate limit exceeded")
        return None
    except Exception as e:
        print(f"  âœ— [{owner}/{repo_name} {month_key}] Error: {e}")
        return None

# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
def collect_repo_data(github, owner, repo_name, start_date, collect_reviews=False, collect_commit_stats=True, use_cache=True, max_workers=3, github_token=None):
    """ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆPRã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã®ã¿ã€ã‚³ãƒŸãƒƒãƒˆã¯åˆ¥é€”ä¸¦åˆ—å‡¦ç†ï¼‰"""
    print(f"\n{'='*60}")
    print(f"Collecting data for {owner}/{repo_name}...")
    start_time = time.time()

    cache_path = get_cache_path(owner, repo_name)
    current_month_start = get_current_month_start()
    cached_data = None

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºå®šåˆ†ã‚’èª­ã¿è¾¼ã¿
    if use_cache:
        cached_data = load_cache(cache_path)
        if cached_data:
            print(f"  ğŸ“¦ Loaded cache (last updated: {cached_data.get('cached_at', 'unknown')})")

    try:
        repo = github.get_repo(f"{owner}/{repo_name}")
    except GithubException as e:
        if e.status == 401:
            print(f"Error accessing {owner}/{repo_name}: Authentication failed (401)")
            print("  The token may not have access to this repository, or the token is invalid.")
        elif e.status == 403:
            print(f"Error accessing {owner}/{repo_name}: Access forbidden (403)")
            print("  The token may not have sufficient permissions, or the repository is private and the token lacks 'repo' scope.")
        elif e.status == 404:
            print(f"Error accessing {owner}/{repo_name}: Repository not found (404)")
            print("  Please check if the repository name and owner are correct.")
        else:
            print(f"Error accessing {owner}/{repo_name}: {e}")
        return None

    data = {
        'repository': f"{owner}/{repo_name}",
        'prs': [],
        'code_frequency': defaultdict(lambda: {'additions': 0, 'deletions': 0}),
        'contributions': defaultdict(lambda: {
            'commits': 0,
            'additions': 0,
            'deletions': 0,
            'prs_created': 0,
            'prs_merged': 0,
            'prs_reviewed': 0
        }),
        'monthly_stats': defaultdict(lambda: {
            'prs_created': 0,
            'prs_merged': 0,
            'additions': 0,
            'deletions': 0,
            'contributors': set()
        }),
        'monthly_contributions': defaultdict(lambda: defaultdict(lambda: {
            'commits': 0,
            'additions': 0,
            'deletions': 0,
            'prs_created': 0,
            'prs_merged': 0,
            'prs_reviewed': 0
        })),
        'devin_breakdown': defaultdict(lambda: {
            'prs_merged': 0,
            'additions': 0,
            'deletions': 0
        })
    }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºå®šåˆ†ã®PRã‚’èª­ã¿è¾¼ã¿
    cached_prs = []
    cached_contributions = defaultdict(lambda: {
        'commits': 0,
        'additions': 0,
        'deletions': 0,
        'prs_created': 0,
        'prs_merged': 0,
        'prs_reviewed': 0
    })
    cached_monthly_stats = defaultdict(lambda: {
        'prs_created': 0,
        'prs_merged': 0,
        'additions': 0,
        'deletions': 0,
        'contributors': set()
    })
    cached_monthly_contributions = defaultdict(lambda: defaultdict(lambda: {
        'commits': 0,
        'additions': 0,
        'deletions': 0,
        'prs_created': 0,
        'prs_merged': 0,
        'prs_reviewed': 0
    }))
    cached_code_frequency = defaultdict(lambda: {'additions': 0, 'deletions': 0})
    cached_devin_breakdown = defaultdict(lambda: {
        'prs_merged': 0,
        'additions': 0,
        'deletions': 0
    })

    if cached_data and use_cache:
        # ç¢ºå®šåˆ†ï¼ˆå½“æœˆã‚ˆã‚Šå‰ï¼‰ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        # start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        cache_has_old_data = False
        for cached_pr in cached_data.get('prs', []):
            pr_created = parser.parse(cached_pr['created_at'])
            if pr_created < current_month_start:
                cached_prs.append(cached_pr)
                # start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
                if pr_created >= start_date:
                    cache_has_old_data = True

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€é€šå¸¸é€šã‚Šå–å¾—ã™ã‚‹
        if not cache_has_old_data and len(cached_prs) > 0:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€ã‚‚å¤ã„PRã®æ—¥ä»˜ã‚’ç¢ºèª
            oldest_cached_pr_date = min(parser.parse(pr['created_at']) for pr in cached_prs)
            if oldest_cached_pr_date > start_date:
                print(f"  âš ï¸  Cache doesn't contain data before {start_date.strftime('%Y-%m-%d')}, will fetch from API")
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦é€šå¸¸é€šã‚Šå–å¾—
                cached_prs = []
                cached_contributions = defaultdict(lambda: {
                    'commits': 0,
                    'additions': 0,
                    'deletions': 0,
                    'prs_created': 0,
                    'prs_merged': 0,
                    'prs_reviewed': 0
                })
                cached_monthly_stats = defaultdict(lambda: {
                    'prs_created': 0,
                    'prs_merged': 0,
                    'additions': 0,
                    'deletions': 0,
                    'contributors': set()
                })
                cached_monthly_contributions = defaultdict(lambda: defaultdict(lambda: {
                    'commits': 0,
                    'additions': 0,
                    'deletions': 0,
                    'prs_created': 0,
                    'prs_merged': 0,
                    'prs_reviewed': 0
                }))
                cached_code_frequency = defaultdict(lambda: {'additions': 0, 'deletions': 0})
                cached_devin_breakdown = defaultdict(lambda: {
                    'prs_merged': 0,
                    'additions': 0,
                    'deletions': 0
                })

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿çµ±è¨ˆã‚’èª­ã¿è¾¼ã¿
        if len(cached_prs) > 0:
            # ç¢ºå®šåˆ†ã®çµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            for month, stats in cached_data.get('monthly_stats', {}).items():
                month_date = parser.parse(f"{month}-01")
                if month_date.tzinfo is None:
                    month_date = JST.localize(month_date)
                if month_date < current_month_start:
                    cached_monthly_stats[month] = stats.copy()
                    if isinstance(stats.get('contributors'), int):
                        cached_monthly_stats[month]['contributors'] = set()

            for month, freq in cached_data.get('code_frequency', {}).items():
                month_date = parser.parse(f"{month}-01")
                if month_date.tzinfo is None:
                    month_date = JST.localize(month_date)
                if month_date < current_month_start:
                    cached_code_frequency[month] = freq.copy()

            # ç¢ºå®šåˆ†ã®ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼çµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            for contributor, stats in cached_data.get('contributions', {}).items():
                cached_contributions[contributor] = stats.copy()

            # æœˆåˆ¥ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼çµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
            for month, contributors in cached_data.get('monthly_contributions', {}).items():
                month_date = parser.parse(f"{month}-01")
                if month_date.tzinfo is None:
                    month_date = JST.localize(month_date)
                if month_date < current_month_start:
                    for contributor, stats in contributors.items():
                        cached_monthly_contributions[month][contributor] = stats.copy()

            # devin-botã®å†…è¨³ã‚‚èª­ã¿è¾¼ã¿
            for contributor, breakdown in cached_data.get('devin_breakdown', {}).items():
                cached_devin_breakdown[contributor] = breakdown.copy()

            print(f"  ğŸ“¦ Using {len(cached_prs)} cached PRs (before {current_month_start.strftime('%Y-%m')})")
        else:
            print(f"  ğŸ“¦ Cache cleared, will fetch all data from API")

    # PRãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆGraphQLã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€å¾“æ¥ã®REST APIã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼‰
    use_graphql = os.getenv('USE_GRAPHQL', 'true').lower() == 'true'

    if use_graphql and github_token:
        # GraphQLã§PRã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¸€æ‹¬å–å¾—
        print(f"  ğŸ”„ Fetching PRs with GraphQL...")
        print(f"  ğŸ“… Start date: {start_date} (timezone: {start_date.tzinfo})")
        try:
            graphql_prs = fetch_prs_with_graphql(github_token, owner, repo_name, start_date, collect_reviews)
            print(f"  âœ“ Fetched {len(graphql_prs)} PRs with GraphQL")
            if len(graphql_prs) > 0:
                print(f"  ğŸ“Š Sample PR: #{graphql_prs[0].get('number')} created_at={graphql_prs[0].get('created_at')}, state={graphql_prs[0].get('state')}")
            else:
                print(f"  âš ï¸  No PRs fetched - checking if repository has PRs...")

            # GraphQLã§å–å¾—ã—ãŸPRã‚’å‡¦ç†
            pr_count = 0
            new_pr_count = 0
            pr_data_map = {}
            determined_prs = []
            last_cache_save_time = time.time()
            cache_save_interval = 30

            # start_dateã‚’UTCã«å¤‰æ›ï¼ˆå‡¦ç†ãƒ«ãƒ¼ãƒ—å†…ã§ä½¿ç”¨ï¼‰
            start_date_utc_for_processing = start_date
            if start_date.tzinfo == JST:
                start_date_utc_for_processing = start_date.astimezone(pytz.UTC)
            elif start_date.tzinfo is None:
                start_date_utc_for_processing = pytz.UTC.localize(start_date)

            for pr_data in graphql_prs:
                pr_created = parser.parse(pr_data['created_at'])
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãŒNoneã®å ´åˆã¯UTCã¨ã—ã¦æ‰±ã†
                if pr_created.tzinfo is None:
                    pr_created = pytz.UTC.localize(pr_created)

                is_determined = pr_created < current_month_start

                if is_determined:
                    if len(cached_prs) > 0:
                        continue
                    if pr_created < start_date_utc_for_processing:
                        continue

                # ç¢ºå®šåˆ†ã®PRã¯é †æ¬¡ä¿å­˜ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ 
                if is_determined:
                    determined_prs.append(pr_data)

                # ç¢ºå®šåˆ†ã®PRã‚’å®šæœŸçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if is_determined and use_cache and len(determined_prs) > 0:
                    current_time = time.time()
                    if current_time - last_cache_save_time >= cache_save_interval:
                        update_pr_cache(cache_path, determined_prs, start_date)
                        print(f"  ğŸ’¾ Saved {len(determined_prs)} determined PRs to cache (interim save)")
                        determined_prs = []
                        last_cache_save_time = current_time

                pr_data_map[pr_data['number']] = pr_data
                data['prs'].append(pr_data)
                pr_count += 1
                new_pr_count += 1

                # çµ±è¨ˆã‚’æ›´æ–°
                month_key = get_month_key(pr_data['created_at'])
                data['monthly_stats'][month_key]['prs_created'] += 1
                if pr_data['merged_at']:
                    merge_month = get_month_key(pr_data['merged_at'])
                    data['monthly_stats'][merge_month]['prs_merged'] += 1

                # devin-ai-integration[bot]ã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
                author = pr_data['author']
                is_devin_bot = author == 'devin-ai-integration[bot]'
                merged_by = pr_data.get('merged_by')

                if is_devin_bot and pr_data['merged_at'] and merged_by:
                    merger = merged_by
                    data['contributions'][merger]['prs_merged'] += 1
                    data['contributions'][merger]['additions'] += pr_data['additions']
                    data['contributions'][merger]['deletions'] += pr_data['deletions']
                    merge_month = get_month_key(pr_data['merged_at'])
                    data['monthly_contributions'][merge_month][merger]['prs_merged'] += 1
                    data['monthly_contributions'][merge_month][merger]['additions'] += pr_data['additions']
                    data['monthly_contributions'][merge_month][merger]['deletions'] += pr_data['deletions']

                    if merger not in data['devin_breakdown']:
                        data['devin_breakdown'][merger] = {
                            'prs_merged': 0,
                            'additions': 0,
                            'deletions': 0
                        }
                    data['devin_breakdown'][merger]['prs_merged'] += 1
                    data['devin_breakdown'][merger]['additions'] += pr_data['additions']
                    data['devin_breakdown'][merger]['deletions'] += pr_data['deletions']
                else:
                    data['contributions'][author]['prs_created'] += 1
                    data['contributions'][author]['additions'] += pr_data['additions']
                    data['contributions'][author]['deletions'] += pr_data['deletions']
                    data['monthly_contributions'][month_key][author]['prs_created'] += 1
                    data['monthly_contributions'][month_key][author]['additions'] += pr_data['additions']
                    data['monthly_contributions'][month_key][author]['deletions'] += pr_data['deletions']

                    if pr_data['merged_at']:
                        merge_month = get_month_key(pr_data['merged_at'])
                        data['contributions'][author]['prs_merged'] += 1
                        data['monthly_contributions'][merge_month][author]['prs_merged'] += 1

                # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®çµ±è¨ˆã‚’æ›´æ–°
                if collect_reviews and pr_data.get('reviewers'):
                    for reviewer in pr_data['reviewers']:
                        data['contributions'][reviewer]['prs_reviewed'] += 1
                        data['monthly_contributions'][month_key][reviewer]['prs_reviewed'] += 1

            # æ®‹ã‚Šã®ç¢ºå®šåˆ†ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if use_cache and len(determined_prs) > 0:
                update_pr_cache(cache_path, determined_prs, start_date)
                print(f"  ğŸ’¾ Saved {len(determined_prs)} determined PRs to cache (final save)")

            print(f"  âœ“ Collected {new_pr_count} new PRs (total: {pr_count + len(cached_prs)} with cache)")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã PRã‚’è¿½åŠ 
            data['prs'].extend(cached_prs)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã PRã®çµ±è¨ˆã‚‚æ›´æ–°
            for cached_pr in cached_prs:
                month_key = get_month_key(cached_pr['created_at'])
                data['monthly_stats'][month_key]['prs_created'] += 1
                if cached_pr.get('merged_at'):
                    merge_month = get_month_key(cached_pr['merged_at'])
                    data['monthly_stats'][merge_month]['prs_merged'] += 1

                author = cached_pr.get('author', 'unknown')
                is_devin_bot = author == 'devin-ai-integration[bot]'
                merged_by = cached_pr.get('merged_by')

                # devin-ai-integration[bot]ã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
                if is_devin_bot and cached_pr.get('merged_at') and merged_by:
                    merger = merged_by
                    data['contributions'][merger]['prs_merged'] += 1
                    data['contributions'][merger]['additions'] += cached_pr.get('additions', 0)
                    data['contributions'][merger]['deletions'] += cached_pr.get('deletions', 0)
                    merge_month = get_month_key(cached_pr['merged_at'])
                    data['monthly_contributions'][merge_month][merger]['prs_merged'] += 1
                    data['monthly_contributions'][merge_month][merger]['additions'] += cached_pr.get('additions', 0)
                    data['monthly_contributions'][merge_month][merger]['deletions'] += cached_pr.get('deletions', 0)

                    if merger not in data['devin_breakdown']:
                        data['devin_breakdown'][merger] = {
                            'prs_merged': 0,
                            'additions': 0,
                            'deletions': 0
                        }
                    data['devin_breakdown'][merger]['prs_merged'] += 1
                    data['devin_breakdown'][merger]['additions'] += cached_pr.get('additions', 0)
                    data['devin_breakdown'][merger]['deletions'] += cached_pr.get('deletions', 0)
                else:
                    # é€šå¸¸ã®PRã®çµ±è¨ˆ
                    data['contributions'][author]['prs_created'] += 1
                    data['contributions'][author]['additions'] += cached_pr.get('additions', 0)
                    data['contributions'][author]['deletions'] += cached_pr.get('deletions', 0)
                    data['monthly_contributions'][month_key][author]['prs_created'] += 1
                    data['monthly_contributions'][month_key][author]['additions'] += cached_pr.get('additions', 0)
                    data['monthly_contributions'][month_key][author]['deletions'] += cached_pr.get('deletions', 0)

                    if cached_pr.get('merged_at'):
                        merge_month = get_month_key(cached_pr['merged_at'])
                        data['contributions'][author]['prs_merged'] += 1
                        data['monthly_contributions'][merge_month][author]['prs_merged'] += 1

                if collect_reviews and cached_pr.get('reviewers'):
                    for reviewer in cached_pr['reviewers']:
                        data['contributions'][reviewer]['prs_reviewed'] += 1
                        data['monthly_contributions'][month_key][reviewer]['prs_reviewed'] += 1

            # GraphQLã§å–å¾—ã—ãŸå ´åˆã¯ã€å¾“æ¥ã®REST APIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ï¼ˆã“ã®å¾Œã€ã‚³ãƒŸãƒƒãƒˆçµ±è¨ˆã®å‡¦ç†ã«é€²ã‚€ï¼‰
        except Exception as e:
            print(f"  âš ï¸  GraphQL fetch failed, falling back to REST API: {e}")
            use_graphql = False  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    if not use_graphql:
        # å¾“æ¥ã®REST APIã‚’ä½¿ç”¨
        try:
            check_rate_limit(github)
            prs = repo.get_pulls(state='all', sort='updated', direction='desc')
        except Exception as e:
            print(f"  âœ— Error getting PRs: {e}")
            prs = []

        if prs:  # prsãŒç©ºã§ãªã„å ´åˆã®ã¿å‡¦ç†
            try:
                pr_count = 0
                new_pr_count = 0
                last_progress_time = time.time()
                progress_interval = 60  # 60ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º
                total_checked = 0  # start_dateä»¥é™ã®PRã‚’ãƒã‚§ãƒƒã‚¯ã—ãŸæ•°

                # PRã®åŸºæœ¬æƒ…å ±ã‚’å…ˆã«åé›†ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å¾Œã§ä¸¦åˆ—å–å¾—ï¼‰
                prs_to_fetch_reviews = []  # ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ãŒå¿…è¦ãªPRã®ãƒªã‚¹ãƒˆ
                pr_data_map = {}  # PRç•ªå· -> PRãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ”ãƒ³ã‚°
                determined_prs = []  # ç¢ºå®šåˆ†ã®PRï¼ˆé †æ¬¡ä¿å­˜ç”¨ï¼‰
                last_cache_save_time = time.time()
                cache_save_interval = 30  # 30ç§’ã”ã¨ã«ç¢ºå®šåˆ†ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜

                for pr in prs:
                    # ç›´è¿‘1å¹´é–“ã®PRã®ã¿å‡¦ç†
                    if pr.updated_at < start_date:
                        break

                    total_checked += 1  # start_dateä»¥é™ã®PRã‚’ãƒã‚§ãƒƒã‚¯

                    # ç¢ºå®šåˆ†ï¼ˆå½“æœˆã‚ˆã‚Šå‰ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
                    # ãŸã ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã‚„ã€start_dateã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯å–å¾—ã™ã‚‹
                    pr_created = pr.created_at
                    is_determined = pr_created < current_month_start
                    if is_determined:
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if len(cached_prs) > 0:
                            continue
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ã€start_dateä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        if pr_created < start_date:
                            continue

                    # ãƒãƒ¼ã‚¸ã—ãŸäººã‚’å–å¾—ï¼ˆdevin-botã®PRã®å ´åˆã«ä½¿ç”¨ï¼‰
                    merged_by = None
                    if pr.merged_at and pr.merged_by:
                        merged_by = pr.merged_by.login

                    pr_data = {
                        'number': pr.number,
                        'title': pr.title,
                        'author': pr.user.login if pr.user else 'unknown',
                        'state': pr.state,
                        'created_at': pr.created_at.isoformat(),
                        'merged_at': pr.merged_at.isoformat() if pr.merged_at else None,
                        'merged_by': merged_by,
                        'additions': pr.additions,
                        'deletions': pr.deletions,
                        'reviewers': []
                    }

                    # ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ãŒå¿…è¦ãªå ´åˆã¯å¾Œã§ä¸¦åˆ—å‡¦ç†ã™ã‚‹ãŸã‚ã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    if collect_reviews:
                        prs_to_fetch_reviews.append((pr.number, pr))

                    pr_data_map[pr.number] = pr_data
                    data['prs'].append(pr_data)
                    pr_count += 1
                    new_pr_count += 1

                    # ç¢ºå®šåˆ†ã®PRã¯é †æ¬¡ä¿å­˜ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    if is_determined:
                        determined_prs.append(pr_data)

                    # ç¢ºå®šåˆ†ã®PRã‚’å®šæœŸçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆå‡¦ç†ä¸­æ–­ã«å‚™ãˆã‚‹ï¼‰
                    if is_determined and use_cache and len(determined_prs) > 0:
                        current_time = time.time()
                        if current_time - last_cache_save_time >= cache_save_interval:
                            update_pr_cache(cache_path, determined_prs, start_date)
                            print(f"  ğŸ’¾ Saved {len(determined_prs)} determined PRs to cache (interim save)")
                            determined_prs = []  # ä¿å­˜æ¸ˆã¿ã®PRã¯ã‚¯ãƒªã‚¢
                            last_cache_save_time = current_time

                    # é€²æ—è¡¨ç¤ºï¼ˆ1åˆ†ã”ã¨ï¼‰
                    current_time = time.time()
                    if current_time - last_progress_time >= progress_interval:
                        elapsed = int(current_time - start_time)
                        elapsed_min = elapsed // 60
                        elapsed_sec = elapsed % 60

                        # å‡¦ç†é€Ÿåº¦ã‚’è¨ˆç®—ï¼ˆPR/ç§’ï¼‰
                        if elapsed > 0:
                            rate = new_pr_count / elapsed
                            # é€²æ—ç‡ã‚’è¨ˆç®—ï¼ˆtotal_checkedã‹ã‚‰æ¨å®šï¼‰
                            if total_checked > 0:
                                progress_pct = min(100, int((new_pr_count / total_checked) * 100))
                                # æ®‹ã‚Šæ™‚é–“ã‚’æ¨å®š
                                remaining = max(0, total_checked - new_pr_count)
                                if rate > 0 and remaining > 0:
                                    eta_seconds = int(remaining / rate)
                                    eta_min = eta_seconds // 60
                                    eta_sec = eta_seconds % 60
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s, ETA: ~{eta_min}m {eta_sec}s)")
                                    else:
                                        print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s, ETA: ~{eta_sec}s)")
                                else:
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                                    else:
                                        print(f"  â³ Progress: {new_pr_count}/{total_checked} PRs collected ({progress_pct}%, elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                            else:
                                # total_checkedãŒ0ã®å ´åˆã¯å¾“æ¥ã®è¡¨ç¤º
                                if rate > 0:
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_min}m {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                                    else:
                                        print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_sec}s, rate: {rate:.2f} PRs/s)")
                                else:
                                    if elapsed_min > 0:
                                        print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_min}m {elapsed_sec}s)")
                                    else:
                                        print(f"  â³ Progress: {new_pr_count} PRs collected (elapsed: {elapsed_sec}s)")
                        else:
                            print(f"  â³ Progress: {new_pr_count} PRs collected")
                        last_progress_time = current_time

                    # æœˆã”ã¨ã®çµ±è¨ˆ
                    month_key = get_month_key(pr.created_at)
                    data['monthly_stats'][month_key]['prs_created'] += 1
                    if pr.merged_at:
                        merge_month = get_month_key(pr.merged_at)
                        data['monthly_stats'][merge_month]['prs_merged'] += 1

                    # devin-ai-integration[bot]ã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã€å®Ÿç¸¾ã‚’ãƒãƒ¼ã‚¸ã—ãŸäººã«è¨ˆä¸Š
                    author = pr.user.login if pr.user else 'unknown'
                    is_devin_bot = author == 'devin-ai-integration[bot]'

                    if is_devin_bot and pr.merged_at and merged_by:
                        # devin-botã®PRãŒãƒãƒ¼ã‚¸ã•ã‚ŒãŸå ´åˆã€ãƒãƒ¼ã‚¸ã—ãŸäººã«å®Ÿç¸¾ã‚’è¨ˆä¸Š
                        merger = merged_by
                        data['contributions'][merger]['prs_merged'] += 1
                        data['contributions'][merger]['additions'] += pr.additions
                        data['contributions'][merger]['deletions'] += pr.deletions
                        data['monthly_stats'][merge_month]['contributors'].add(merger)
                        # æœˆåˆ¥çµ±è¨ˆ
                        data['monthly_contributions'][merge_month][merger]['prs_merged'] += 1
                        data['monthly_contributions'][merge_month][merger]['additions'] += pr.additions
                        data['monthly_contributions'][merge_month][merger]['deletions'] += pr.deletions

                        # devin-botã®å†…è¨³ã‚‚è¨˜éŒ²ï¼ˆæ‹¬å¼§æ›¸ãè¡¨ç¤ºç”¨ï¼‰
                        if 'devin_breakdown' not in data:
                            data['devin_breakdown'] = defaultdict(lambda: {
                                'prs_merged': 0,
                                'additions': 0,
                                'deletions': 0
                            })
                        data['devin_breakdown'][merger]['prs_merged'] += 1
                        data['devin_breakdown'][merger]['additions'] += pr.additions
                        data['devin_breakdown'][merger]['deletions'] += pr.deletions
                    else:
                        # é€šå¸¸ã®PRã®çµ±è¨ˆ
                        if pr.user:
                            data['contributions'][author]['prs_created'] += 1
                            if pr.merged_at:
                                data['contributions'][author]['prs_merged'] += 1
                            data['contributions'][author]['additions'] += pr.additions
                            data['contributions'][author]['deletions'] += pr.deletions
                            data['monthly_stats'][month_key]['contributors'].add(author)
                            # æœˆåˆ¥çµ±è¨ˆ
                            data['monthly_contributions'][month_key][author]['prs_created'] += 1
                            data['monthly_contributions'][month_key][author]['additions'] += pr.additions
                            data['monthly_contributions'][month_key][author]['deletions'] += pr.deletions
                            if pr.merged_at:
                                data['monthly_contributions'][merge_month][author]['prs_merged'] += 1

                # æ®‹ã‚Šã®ç¢ºå®šåˆ†ã®PRã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if use_cache and len(determined_prs) > 0:
                    update_pr_cache(cache_path, determined_prs, start_date)
                    print(f"  ğŸ’¾ Saved {len(determined_prs)} determined PRs to cache (final save)")

                print(f"  âœ“ Collected {new_pr_count} new PRs (total: {pr_count + len(cached_prs)} with cache)")

                # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¸¦åˆ—å–å¾—ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ãŒæœ‰åŠ¹ãªå ´åˆã€PRã®åŸºæœ¬æƒ…å ±åé›†å¾Œã«å®Ÿè¡Œï¼‰
                if collect_reviews and prs_to_fetch_reviews:
                    print(f"  ğŸ”„ Fetching reviews for {len(prs_to_fetch_reviews)} PRs in parallel...")
                    review_workers = min(max_workers, len(prs_to_fetch_reviews))
                    review_start_time = time.time()
                    with ThreadPoolExecutor(max_workers=review_workers) as executor:
                        future_to_pr = {
                            executor.submit(fetch_pr_reviews, github, pr_number, pr): pr_number
                            for pr_number, pr in prs_to_fetch_reviews
                        }

                        completed = 0
                        for future in as_completed(future_to_pr):
                            pr_number = future_to_pr[future]
                            completed += 1
                            try:
                                _, reviewers = future.result()
                                if pr_number in pr_data_map:
                                    pr_data_map[pr_number]['reviewers'] = reviewers
                                    # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®çµ±è¨ˆã‚’æ›´æ–°
                                    pr_data = pr_data_map[pr_number]
                                    month_key = get_month_key(pr_data['created_at'])
                                    for reviewer in reviewers:
                                        data['contributions'][reviewer]['prs_reviewed'] += 1
                                        data['monthly_contributions'][month_key][reviewer]['prs_reviewed'] += 1
                            except Exception as e:
                                print(f"  âš ï¸  Error fetching reviews for PR #{pr_number}: {e}")
                                if pr_number in pr_data_map:
                                    pr_data_map[pr_number]['reviewers'] = []

                            # é€²æ—è¡¨ç¤ºï¼ˆ10ä»¶ã”ã¨ï¼‰
                            if completed % 10 == 0:
                                elapsed = time.time() - review_start_time
                                rate = completed / elapsed if elapsed > 0 else 0
                                remaining = len(prs_to_fetch_reviews) - completed
                                eta = remaining / rate if rate > 0 else 0
                                print(f"  â³ Reviews: {completed}/{len(prs_to_fetch_reviews)} ({rate:.1f} PRs/s, ETA: {int(eta)}s)")

                    review_elapsed = time.time() - review_start_time
                    print(f"  âœ“ Fetched reviews for {len(prs_to_fetch_reviews)} PRs in {review_elapsed:.1f}s")

            except RateLimitExceededException:
                print(f"  âš ï¸  Rate limit exceeded while fetching PRs")
            except Exception as e:
                print(f"  âœ— Error collecting PRs: {e}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã PRã‚’è¿½åŠ 
        data['prs'].extend(cached_prs)

    # Code frequencyãƒ‡ãƒ¼ã‚¿ã®åé›†ã¯mainé–¢æ•°ã§ä¸¦åˆ—å‡¦ç†ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ã ã‘
    # ã‚³ãƒŸãƒƒãƒˆçµ±è¨ˆã®åé›†ã¯mainé–¢æ•°ã§æœˆã”ã¨ã«ä¸¦åˆ—å‡¦ç†ã•ã‚Œã‚‹

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã çµ±è¨ˆã‚’ãƒãƒ¼ã‚¸
    for contributor, stats in cached_contributions.items():
        for key, value in stats.items():
            data['contributions'][contributor][key] += value

    for month, stats in cached_monthly_stats.items():
        for key, value in stats.items():
            if key == 'contributors':
                if isinstance(value, set):
                    data['monthly_stats'][month]['contributors'].update(value)
                else:
                    # æ•°å€¤ã®å ´åˆã¯ç„¡è¦–ï¼ˆå¾Œã§è¨ˆç®—ã—ç›´ã™ï¼‰
                    pass
            else:
                data['monthly_stats'][month][key] += value

    # æœˆåˆ¥ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼çµ±è¨ˆã‚’ãƒãƒ¼ã‚¸
    for month, contributors in cached_monthly_contributions.items():
        for contributor, stats in contributors.items():
            for key, value in stats.items():
                data['monthly_contributions'][month][contributor][key] += value

    for contributor, breakdown in cached_devin_breakdown.items():
        if 'devin_breakdown' not in data:
            data['devin_breakdown'] = defaultdict(lambda: {
                'prs_merged': 0,
                'additions': 0,
                'deletions': 0
            })
        for key, value in breakdown.items():
            data['devin_breakdown'][contributor][key] += value

    # ã‚»ãƒƒãƒˆã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
    for month_key in data['monthly_stats']:
        if isinstance(data['monthly_stats'][month_key]['contributors'], set):
            data['monthly_stats'][month_key]['contributors'] = len(data['monthly_stats'][month_key]['contributors'])

    # è¾æ›¸ã‚’é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›
    data['code_frequency'] = dict(data['code_frequency'])
    data['contributions'] = dict(data['contributions'])
    data['monthly_stats'] = dict(data['monthly_stats'])
    # monthly_contributionsã‚’é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›
    monthly_contributions_dict = {}
    for month, contributors in data['monthly_contributions'].items():
        monthly_contributions_dict[month] = dict(contributors)
    data['monthly_contributions'] = monthly_contributions_dict
    if 'devin_breakdown' in data:
        data['devin_breakdown'] = dict(data['devin_breakdown'])

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®ãŸã‚ã«ï¼‰
    if use_cache:
        cache_data = {
            'cached_at': datetime.now(JST).isoformat(),
            'start_date': start_date.isoformat(), # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é–‹å§‹æ—¥ã‚’ä¿å­˜
            'repository': data['repository'],
            'prs': data['prs'],
            'contributions': data['contributions'],
            'monthly_stats': data['monthly_stats'],
            'monthly_contributions': data.get('monthly_contributions', {}),
            'code_frequency': data['code_frequency'],
            'devin_breakdown': data.get('devin_breakdown', {})
        }
        save_cache(cache_path, cache_data)
        print(f"  ğŸ’¾ Cache saved for next run")

    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    if minutes > 0:
        print(f"  âœ“ Completed in {minutes}m {seconds}s")
    else:
        print(f"  âœ“ Completed in {elapsed_time:.1f}s")
    print(f"{'='*60}\n")

    return data

def main():
    # GitHub PATã‚’å–å¾—
    github_token = os.getenv('GITHUB_TOKEN')
    if not github_token:
        print("Error: GITHUB_TOKEN environment variable is not set")
        print("Please set GITHUB_TOKEN environment variable or use GitHub Actions secrets")
        print("You can create a token at: https://github.com/settings/tokens")
        sys.exit(1)

    # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦Githubã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆæ–°ã—ã„APIã‚’ä½¿ç”¨ï¼‰
    auth = Auth.Token(github_token)
    github = Github(auth=auth, per_page=100)

    # ãƒªãƒã‚¸ãƒˆãƒªè¨­å®šã‚’èª­ã¿è¾¼ã¿
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'repos.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿
    options = config.get('options', {})
    collect_reviews = options.get('collect_reviews', False)
    collect_commit_stats = options.get('collect_commit_stats', True)
    max_workers = options.get('max_workers', 3)
    use_cache = options.get('use_cache', True)

    # å¯¾è±¡æœŸé–“ã®è¨­å®š
    # days: ä½•æ—¥å‰ã‹ã‚‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 365æ—¥ = 1å¹´ï¼‰
    # start_date: é–‹å§‹æ—¥ã‚’ISOå½¢å¼ã§æŒ‡å®šï¼ˆä¾‹: "2024-01-01T00:00:00Z"ï¼‰
    # start_dateãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆ
    if 'start_date' in options:
        try:
            start_date = parser.parse(options['start_date'])
            if start_date.tzinfo is None:
                start_date = JST.localize(start_date)
            print(f"Using custom start date: {start_date.isoformat()}")
        except Exception as e:
            print(f"Warning: Invalid start_date format, using days option instead: {e}")
            days = options.get('days', 365)
            start_date = get_start_date(days)
    else:
        days = options.get('days', 365)
        start_date = get_start_date(days)
        print(f"Using {days} days period (from {start_date.isoformat()})")

    all_data = []

    repos = config['repositories']
    total_repos = len(repos)

    # æœ€åˆã®ãƒªãƒã‚¸ãƒˆãƒªã§èªè¨¼ã‚’ç¢ºèª
    if repos:
        first_repo = repos[0]
        try:
            test_repo = github.get_repo(f"{first_repo['owner']}/{first_repo['name']}")
            print(f"âœ“ Authentication successful (testing with {first_repo['owner']}/{first_repo['name']})")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤º
            rate_limit = github.get_rate_limit()
            if hasattr(rate_limit, 'resources'):
                core_limit = rate_limit.resources.core
                print(f"Rate limit: {core_limit.remaining}/{core_limit.limit} (resets at {core_limit.reset})")
            else:
                print(f"Rate limit: {rate_limit.core.remaining}/{rate_limit.core.limit} (resets at {rate_limit.core.reset})")
        except GithubException as e:
            if e.status == 401:
                print("Error: Invalid GitHub token (401 Unauthorized)")
                print("Please check your GITHUB_TOKEN:")
                print("1. Token is valid and not expired")
                print("2. Token has necessary permissions (repo scope for private repos)")
                print("3. Token is correctly set in environment variable")
                print("You can create a new token at: https://github.com/settings/tokens")
            elif e.status == 403:
                print("Error: Access forbidden (403 Forbidden)")
                print("Please check your GITHUB_TOKEN:")
                print("1. Token has sufficient permissions (repo scope for private repos)")
                print("2. Token is not rate limited")
                print("3. Token has access to the requested resources")
                print(f"4. Token has access to {first_repo['owner']}/{first_repo['name']}")
                print("You can check token permissions at: https://github.com/settings/tokens")
            else:
                print(f"Error accessing {first_repo['owner']}/{first_repo['name']}: {e}")
            sys.exit(1)

    print(f"\n{'='*60}")
    print(f"Processing {total_repos} repository/repositories...")
    print(f"Options: collect_reviews={collect_reviews}, collect_commit_stats={collect_commit_stats}, max_workers={max_workers}, use_cache={use_cache}")
    print(f"Period: {start_date.isoformat()} to {datetime.now(JST).isoformat()}")
    print(f"{'='*60}\n")

    # ã¾ãšå„ãƒªãƒã‚¸ãƒˆãƒªã®PRãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    repo_data_map = {}
    if total_repos > 1 and max_workers > 1:
        print(f"Using parallel processing for PRs (max {max_workers} workers)...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_repo = {
                executor.submit(
                    collect_repo_data,
                    github,
                    repo_config['owner'],
                    repo_config['name'],
                    start_date,
                    collect_reviews,
                    collect_commit_stats,
                    use_cache,
                    max_workers,
                    github_token
                ): repo_config
                for repo_config in repos
            }

            for future in as_completed(future_to_repo):
                repo_config = future_to_repo[future]
                try:
                    repo_data = future.result()
                    if repo_data:
                        repo_key = f"{repo_config['owner']}/{repo_config['name']}"
                        repo_data_map[repo_key] = repo_data
                except Exception as e:
                    print(f"Error processing {repo_config['owner']}/{repo_config['name']}: {e}")
    else:
        for repo_config in repos:
            owner = repo_config['owner']
            name = repo_config['name']
            repo_data = collect_repo_data(github, owner, name, start_date, collect_reviews, collect_commit_stats, use_cache, max_workers, github_token)
            if repo_data:
                repo_key = f"{owner}/{name}"
                repo_data_map[repo_key] = repo_data

    # ã‚³ãƒŸãƒƒãƒˆçµ±è¨ˆã‚’åé›†ã™ã‚‹å ´åˆã€æœˆã”ã¨ã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
    if collect_commit_stats:
        # å…¨ãƒªãƒã‚¸ãƒˆãƒªÃ—å…¨æœˆã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        month_tasks = []
        for repo_key, repo_data in repo_data_map.items():
            owner, repo_name = repo_key.split('/')
            cache_path = get_cache_path(owner, repo_name)

            # å¿…è¦ãªæœˆã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
            months_to_process = []
            current = datetime(start_date.year, start_date.month, 1, tzinfo=JST)
            now = datetime.now(JST)
            while current <= now:
                month_key = current.strftime('%Y-%m')
                year, month = current.year, current.month
                month_start, month_end = get_month_range(year, month)
                months_to_process.append((month_key, month_start, month_end))
                if month == 12:
                    current = datetime(year + 1, 1, 1, tzinfo=JST)
                else:
                    current = datetime(year, month + 1, 1, tzinfo=JST)

            # å„æœˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€å®Œå…¨ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€
            for month_key, month_start, month_end in months_to_process:
                chunk = load_monthly_chunk(cache_path, month_key) if use_cache else None
                if chunk:
                    chunk_start = parser.parse(chunk.get('start_date', ''))
                    chunk_end = parser.parse(chunk.get('end_date', ''))
                    if chunk_start <= month_start and chunk_end >= month_end:
                        # å®Œå…¨ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯èª­ã¿è¾¼ã‚€
                        print(f"  ğŸ“¦ Using cached chunk for {owner}/{repo_name} {month_key}")
                        if 'code_frequency' in chunk:
                            if month_key in chunk['code_frequency']:
                                repo_data['code_frequency'][month_key] = chunk['code_frequency'][month_key].copy()
                        if 'monthly_stats' in chunk:
                            if month_key in chunk['monthly_stats']:
                                stats = chunk['monthly_stats'][month_key]
                                if month_key not in repo_data['monthly_stats']:
                                    repo_data['monthly_stats'][month_key] = {
                                        'prs_created': 0,
                                        'prs_merged': 0,
                                        'additions': 0,
                                        'deletions': 0,
                                        'contributors': set()
                                    }
                                # contributorsãŒæ—¢ã«æ•°å€¤ã®å ´åˆã¯setã«å¤‰æ›
                                if isinstance(repo_data['monthly_stats'][month_key].get('contributors'), int):
                                    repo_data['monthly_stats'][month_key]['contributors'] = set()
                                if isinstance(stats.get('contributors'), list):
                                    repo_data['monthly_stats'][month_key]['contributors'].update(stats.get('contributors', []))
                                elif isinstance(stats.get('contributors'), int):
                                    # æ—¢ã«æ•°å€¤ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¾Œã§è¨ˆç®—ï¼‰
                                    pass
                                repo_data['monthly_stats'][month_key]['additions'] += stats.get('additions', 0)
                                repo_data['monthly_stats'][month_key]['deletions'] += stats.get('deletions', 0)
                        if 'monthly_contributions' in chunk:
                            if month_key in chunk['monthly_contributions']:
                                if month_key not in repo_data['monthly_contributions']:
                                    repo_data['monthly_contributions'][month_key] = defaultdict(lambda: {
                                        'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                    })
                                for contributor, stats in chunk['monthly_contributions'][month_key].items():
                                    if not contributor:  # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                        continue
                                    if not isinstance(stats, dict):
                                        continue
                                    # contributorã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–
                                    if contributor not in repo_data['monthly_contributions'][month_key]:
                                        repo_data['monthly_contributions'][month_key][contributor] = {
                                            'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                        }
                                    for key, value in stats.items():
                                        # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦ã‹ã‚‰åŠ ç®—
                                        if key not in repo_data['monthly_contributions'][month_key][contributor]:
                                            repo_data['monthly_contributions'][month_key][contributor][key] = 0
                                        repo_data['monthly_contributions'][month_key][contributor][key] += value
                        if 'contributions' in chunk:
                            for contributor, stats in chunk['contributions'].items():
                                if not contributor:  # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                    continue
                                if not isinstance(stats, dict):
                                    continue
                                # contributorã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–
                                if contributor not in repo_data['contributions']:
                                    repo_data['contributions'][contributor] = {
                                        'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                    }
                                for key, value in stats.items():
                                    # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦ã‹ã‚‰åŠ ç®—
                                    if key not in repo_data['contributions'][contributor]:
                                        repo_data['contributions'][contributor][key] = 0
                                    repo_data['contributions'][contributor][key] += value
                        continue
                # ãƒ•ã‚§ãƒƒãƒãŒå¿…è¦ãªæœˆã‚’ã‚¿ã‚¹ã‚¯ã«è¿½åŠ 
                month_tasks.append((owner, repo_name, month_key, month_start, month_end, cache_path))

        # æœˆã”ã¨ã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
        if month_tasks:
            print(f"\nğŸ”„ Fetching commits for {len(month_tasks)} month(s) across {len(repo_data_map)} repository/repositories...")
            print(f"Using parallel processing (max {max_workers} workers)...")
            print(f"  Tasks: {[(owner, repo_name, month_key) for owner, repo_name, month_key, _, _, _ in month_tasks[:5]]}...")

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_task = {
                    executor.submit(
                        fetch_month_commits,
                        github,
                        owner,
                        repo_name,
                        month_key,
                        month_start,
                        month_end,
                        cache_path,
                        use_cache
                    ): (owner, repo_name, month_key)
                    for owner, repo_name, month_key, month_start, month_end, cache_path in month_tasks
                }

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã®çµæœã‚’ãƒãƒ¼ã‚¸
                for future in as_completed(future_to_task):
                    owner, repo_name, month_key = future_to_task[future]
                    repo_key = f"{owner}/{repo_name}"
                    try:
                        result = future.result()
                        if result and repo_key in repo_data_map:
                            repo_data = repo_data_map[repo_key]
                            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
                            if 'month_key' not in result:
                                print(f"  âš ï¸  [{owner}/{repo_name}] Result missing 'month_key', skipping...")
                                continue
                            month_key_result = result['month_key']
                            # code_frequencyã¯{month_key: {...}}ã®å½¢å¼
                            if month_key_result in result.get('code_frequency', {}):
                                if month_key_result not in repo_data['code_frequency']:
                                    repo_data['code_frequency'][month_key_result] = {'additions': 0, 'deletions': 0}
                                repo_data['code_frequency'][month_key_result]['additions'] += result['code_frequency'][month_key_result]['additions']
                                repo_data['code_frequency'][month_key_result]['deletions'] += result['code_frequency'][month_key_result]['deletions']

                            # contributionsãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                            if 'contributions' in result and result['contributions']:
                                for contributor, stats in result['contributions'].items():
                                    if not contributor:  # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                        continue
                                    if not isinstance(stats, dict):
                                        continue
                                    # contributorã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–
                                    if contributor not in repo_data['contributions']:
                                        repo_data['contributions'][contributor] = {
                                            'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                        }
                                    for key, value in stats.items():
                                        # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦ã‹ã‚‰åŠ ç®—
                                        if key not in repo_data['contributions'][contributor]:
                                            repo_data['contributions'][contributor][key] = 0
                                        repo_data['contributions'][contributor][key] += value

                            # monthly_contributionsãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                            monthly_contributions = result.get('monthly_contributions', {})
                            if monthly_contributions and month_key_result in monthly_contributions:
                                month_contribs = monthly_contributions[month_key_result]
                                if isinstance(month_contribs, dict):
                                    # month_key_resultãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–
                                    if month_key_result not in repo_data['monthly_contributions']:
                                        repo_data['monthly_contributions'][month_key_result] = defaultdict(lambda: {
                                            'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                        })
                                    for contributor, stats in month_contribs.items():
                                        if not contributor:  # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                            continue
                                        if not isinstance(stats, dict):
                                            continue
                                        # contributorã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸåŒ–
                                        if contributor not in repo_data['monthly_contributions'][month_key_result]:
                                            repo_data['monthly_contributions'][month_key_result][contributor] = {
                                                'commits': 0, 'additions': 0, 'deletions': 0, 'prs_created': 0, 'prs_merged': 0, 'prs_reviewed': 0
                                            }
                                        for key, value in stats.items():
                                            # å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦ã‹ã‚‰åŠ ç®—
                                            if key not in repo_data['monthly_contributions'][month_key_result][contributor]:
                                                repo_data['monthly_contributions'][month_key_result][contributor][key] = 0
                                            repo_data['monthly_contributions'][month_key_result][contributor][key] += value

                            # contributorsãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
                            contributors = result.get('contributors', [])
                            if contributors and isinstance(contributors, list):
                                for contributor in contributors:
                                    if not contributor:  # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                        continue
                                if month_key_result not in repo_data['monthly_stats']:
                                    repo_data['monthly_stats'][month_key_result] = {
                                        'prs_created': 0, 'prs_merged': 0, 'additions': 0, 'deletions': 0, 'contributors': set()
                                    }
                                if isinstance(repo_data['monthly_stats'][month_key_result]['contributors'], set):
                                    repo_data['monthly_stats'][month_key_result]['contributors'].add(contributor)
                                else:
                                    repo_data['monthly_stats'][month_key_result]['contributors'] = set([contributor])

                            if month_key_result in result.get('code_frequency', {}):
                                if month_key_result not in repo_data['monthly_stats']:
                                    repo_data['monthly_stats'][month_key_result] = {
                                        'prs_created': 0, 'prs_merged': 0, 'additions': 0, 'deletions': 0, 'contributors': set()
                                    }
                                repo_data['monthly_stats'][month_key_result]['additions'] += result['code_frequency'][month_key_result]['additions']
                                repo_data['monthly_stats'][month_key_result]['deletions'] += result['code_frequency'][month_key_result]['deletions']

                            print(f"  âœ“ [{owner}/{repo_name} {month_key_result}] {result['commit_count']} commits")
                    except Exception as e:
                        import traceback
                        print(f"  âœ— Error processing {owner}/{repo_name} {month_key}: {e}")
                        print(f"    Traceback: {traceback.format_exc()}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
    all_data = list(repo_data_map.values())

    # contributorsã‚’setã‹ã‚‰æ•°å€¤ã«å¤‰æ›ï¼ˆJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã®ãŸã‚ï¼‰
    for repo_data in all_data:
        for month_key in repo_data.get('monthly_stats', {}):
            if isinstance(repo_data['monthly_stats'][month_key].get('contributors'), set):
                repo_data['monthly_stats'][month_key]['contributors'] = len(repo_data['monthly_stats'][month_key]['contributors'])
        # monthly_contributionsã‚’é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›
        if 'monthly_contributions' in repo_data:
            monthly_contributions_dict = {}
            for month, contributors in repo_data['monthly_contributions'].items():
                monthly_contributions_dict[month] = dict(contributors)
            repo_data['monthly_contributions'] = monthly_contributions_dict

    # ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'collected_data.json')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'collected_at': datetime.now(JST).isoformat(),
            'start_date': start_date.isoformat(),
            'repositories': all_data
        }, f, indent=2, ensure_ascii=False)

    print(f"\n{'='*60}")
    print(f"Data collection completed. Saved to {output_path}")
    print(f"Total repositories processed: {len(all_data)}/{total_repos}")

    # æœ€çµ‚çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤º
    rate_limit = github.get_rate_limit()
    if hasattr(rate_limit, 'resources'):
        core_limit = rate_limit.resources.core
        print(f"Rate limit remaining: {core_limit.remaining}/{core_limit.limit}")
    else:
        print(f"Rate limit remaining: {rate_limit.core.remaining}/{rate_limit.core.limit}")

if __name__ == '__main__':
    main()
